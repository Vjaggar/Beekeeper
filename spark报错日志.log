Error: org.apache.spark.sql.AnalysisException: `tmp`.`TMP_ASS_EVT_OFR_INST_2` already exists.; (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: `tmp`.`TMP_ASS_EVT_OFR_INST_2` already exists.;
        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)

        

Error: org.apache.spark.sql.AnalysisException: cannot resolve '`a.rowid`' given input columns: [std_region_id, lan_id, cust_order_id, order_item_id, address_desc, std_service_offer_id, dev_staff_id, etl_time, finish_date, std_order_status_cd, src_offer_id, p_lan_id, accept_date, create_post, day_id, prod_inst_id, STATUS_DATE, src_order_status_cd, address_id, create_staff_id, std_prod_id, mon_id, prod_inst_id, std_offer_id, src_prod_id, p_day_id, channel_id, acc_nbr, src_service_offer_id, offer_inst_id]; line 27 pos 6;
'Project [DAY_ID#62683, MON_ID#62684, LAN_ID#62686, ETL_TIME#62685, PROD_INST_ID#62665, STD_PROD_ID#62674, STD_REGION_ID#62679, CHANNEL_ID#62673, ACC_NBR#62675, ADDRESS_DESC#62676, SRC_ORDER_STATUS_CD#62671, SRC_SERVICE_OFFER_ID#62669, 11 AS STD_ORDER_STATUS_CD#62658, STD_SERVICE_OFFER_ID#62670, ORDER_ITEM_ID#62663, OFFER_INST_ID#62666, DEV_STAFF_ID#62677, CREATE_STAFF_ID#62678, ACCEPT_DATE#62680, STATUS_DATE#62690 AS FINISH_DATE#62659, SRC_OFFER_ID#62667, STD_OFFER_ID#62668, CUST_ORDER_ID#62664, SRC_PROD_ID#62682, ... 3 more fields]
+- Filter (((p_day_id#62661 = 20180325) && (p_lan_id#62662 = 11)) && ((lan_id#62686 = 11) && (STD_SERVICE_OFFER_ID#62670 = 10)))
   +- Join Inner, ((prod_inst_id#62665 = prod_inst_id#62689) && (FINISH_DATE#62681 < STATUS_DATE#62690))
      :- SubqueryAlias a
      :  +- MetastoreRelation tst, ass_evt_ofr_inst_d
      +- SubqueryAlias b
         +- Project [prod_inst_id#62689, STATUS_DATE#62690]
            +- Filter (concat(substring(STATUS_DATE#62690, 0, 4), substring(STATUS_DATE#62690, 5, 2), substring(STATUS_DATE#62690, 8, 2)) >= 201712)
               +- MetastoreRelation tmp, tmp_ass_evt_cdma_active_d (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: cannot resolve '`a.rowid`' given input columns: [std_region_id, lan_id, cust_order_id, order_item_id, address_desc, std_service_offer_id, dev_staff_id, etl_time, finish_date, std_order_status_cd, src_offer_id, p_lan_id, accept_date, create_post, day_id, prod_inst_id, STATUS_DATE, src_order_status_cd, address_id, create_staff_id, std_prod_id, mon_id, prod_inst_id, std_offer_id, src_prod_id, p_day_id, channel_id, acc_nbr, src_service_offer_id, offer_inst_id]; line 27 pos 6;
'Project [DAY_ID#62683, MON_ID#62684, LAN_ID#62686, ETL_TIME#62685, PROD_INST_ID#62665, STD_PROD_ID#62674, STD_REGION_ID#62679, CHANNEL_ID#62673, ACC_NBR#62675, ADDRESS_DESC#62676, SRC_ORDER_STATUS_CD#62671, SRC_SERVICE_OFFER_ID#62669, 11 AS STD_ORDER_STATUS_CD#62658, STD_SERVICE_OFFER_ID#62670, ORDER_ITEM_ID#62663, OFFER_INST_ID#62666, DEV_STAFF_ID#62677, CREATE_STAFF_ID#62678, ACCEPT_DATE#62680, STATUS_DATE#62690 AS FINISH_DATE#62659, SRC_OFFER_ID#62667, STD_OFFER_ID#62668, CUST_ORDER_ID#62664, SRC_PROD_ID#62682, ... 3 more fields]
+- Filter (((p_day_id#62661 = 20180325) && (p_lan_id#62662 = 11)) && ((lan_id#62686 = 11) && (STD_SERVICE_OFFER_ID#62670 = 10)))
   +- Join Inner, ((prod_inst_id#62665 = prod_inst_id#62689) && (FINISH_DATE#62681 < STATUS_DATE#62690))
      :- SubqueryAlias a
      :  +- MetastoreRelation tst, ass_evt_ofr_inst_d
      +- SubqueryAlias b
         +- Project [prod_inst_id#62689, STATUS_DATE#62690]
            +- Filter (concat(substring(STATUS_DATE#62690, 0, 4), substring(STATUS_DATE#62690, 5, 2), substring(STATUS_DATE#62690, 8, 2)) >= 201712)
               +- MetastoreRelation tmp, tmp_ass_evt_cdma_active_d

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)



Error: org.apache.spark.sql.catalyst.parser.ParseException: 
extraneous input 'a' expecting {<EOF>, '.', '[', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'OR', 'AND', 'IN', NOT, 'BETWEEN', 'LIKE', RLIKE, 'IS', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', EQ, '<=>', '<>', '!=', '<', LTE, '>', GTE, '+', '-', '*', '/', '%', 'DIV', '&', '|', '^', 'SORT', 'CLUSTER', 'DISTRIBUTE', STRING}(line 35, pos 60)

== SQL ==
CREATE TABLE tmp.TMP_ASS_EVT_OFR_INST_2
  AS
    SELECT DAY_ID
      ,MON_ID
      ,LAN_ID
      ,ETL_TIME
      ,a.PROD_INST_ID
      ,STD_PROD_ID
      ,STD_REGION_ID
      ,CHANNEL_ID
      ,ACC_NBR
      ,ADDRESS_DESC
      ,SRC_ORDER_STATUS_CD
      ,SRC_SERVICE_OFFER_ID
      ,'11' STD_ORDER_STATUS_CD
      ,STD_SERVICE_OFFER_ID
      ,ORDER_ITEM_ID
      ,OFFER_INST_ID
      ,DEV_STAFF_ID
      ,CREATE_STAFF_ID
      ,ACCEPT_DATE
      ,b.STATUS_DATE as FINISH_DATE
      ,SRC_OFFER_ID
      ,STD_OFFER_ID
      ,CUST_ORDER_ID,
      SRC_PROD_ID,
      a.rowid AS row_id,CREATE_POST,ADDRESS_ID
             FROM tst.ASS_EVT_OFR_INST_D  a   --全量数据
            INNER JOIN (SELECT prod_inst_id, STATUS_DATE
                          FROM tmp.TMP_ASS_EVT_CDMA_ACTIVE_D
                         WHERE concat(substr(STATUS_DATE,0,4),substr(STATUS_DATE,5,2),substr(STATUS_DATE,8,2)) >=
                               '201712' ) b
               ON a.prod_inst_id = b.prod_inst_id
              AND a.FINISH_DATE < b.STATUS_DATE
           WHERE A.p_day_id='20180325' and a.p_lan_id='11'  a.lan_id     = '11'
------------------------------------------------------------^^^
             AND a.STD_SERVICE_OFFER_ID  = '10' (state=,code=0)
             
             
Error: org.apache.spark.sql.AnalysisException: Table or view not found: ass_evt_ofr_inst_d; line 1 pos 14 (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: Table or view not found: ass_evt_ofr_inst_d; line 1 pos 14
        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
        
        
Error: org.apache.spark.sql.AnalysisException: org.apache.hadoop.hive.ql.metadata.HiveException: AlreadyExistsException(message:Partition already exists: Partition(values:[20180325, 11], dbName:tst, tableName:ass_evt_ofr_inst_d, createTime:0, lastAccessTime:0, sd:StorageDescriptor(cols:[FieldSchema(name:order_item_id, type:string, comment:null), FieldSchema(name:cust_order_id, type:string, comment:null), FieldSchema(name:prod_inst_id, type:string, comment:null), FieldSchema(name:offer_inst_id, type:string, comment:null), FieldSchema(name:src_offer_id, type:string, comment:null), FieldSchema(name:std_offer_id, type:string, comment:null), FieldSchema(name:src_service_offer_id, type:string, comment:null), FieldSchema(name:std_service_offer_id, type:string, comment:null), FieldSchema(name:src_order_status_cd, type:string, comment:null), FieldSchema(name:std_order_status_cd, type:string, comment:null), FieldSchema(name:channel_id, type:string, comment:null), FieldSchema(name:std_prod_id, type:string, comment:null), FieldSchema(name:acc_nbr, type:string, comment:null), FieldSchema(name:address_desc, type:string, comment:null), FieldSchema(name:dev_staff_id, type:string, comment:null), FieldSchema(name:create_staff_id, type:string, comment:null), FieldSchema(name:std_region_id, type:string, comment:null), FieldSchema(name:accept_date, type:string, comment:null), FieldSchema(name:finish_date, type:string, comment:null), FieldSchema(name:src_prod_id, type:string, comment:null), FieldSchema(name:day_id, type:string, comment:null), FieldSchema(name:mon_id, type:string, comment:null), FieldSchema(name:etl_time, type:string, comment:null), FieldSchema(name:lan_id, type:string, comment:null), FieldSchema(name:create_post, type:string, comment:null), FieldSchema(name:address_id, type:string, comment:null)], location:hdfs://nameservice1/user/hive/warehouse/tst.db/ass_evt_ofr_inst_d/p_day_id=20180325/p_lan_id=11, inputFormat:org.apache.hadoop.mapred.TextInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, parameters:{serialization.format=|, field.delim=|}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{}), storedAsSubDirectories:false), parameters:null)); (state=,code=0)
        
        
        
        
        Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'ass_evt_ofr_inst_d' not found in database 'inf'; (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'ass_evt_ofr_inst_d' not found in database 'inf';
        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
        
    Error: org.apache.spark.sql.AnalysisException: `tmp`.`TMP_ASS_EVT_CDMA_ACTIVE_D` already exists.; (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: `tmp`.`TMP_ASS_EVT_CDMA_ACTIVE_D` already exists.;
        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
0: jdbc:hive2://hnedaint02:10001/default> drop table tmp.TMP_ASS_EVT_CDMA_ACTIVE_D;





Error: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'WHERE' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}(line 1, pos 0)

== SQL ==
WHERE p_month_id='201803' and p_day_id='20180325' and lan_id = '11' limit 5
^^^ (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'WHERE' expecting {'(', 'SELECT', 'FROM', 'ADD', 'DESC', 'WITH', 'VALUES', 'CREATE', 'TABLE', 'INSERT', 'DELETE', 'DESCRIBE', 'EXPLAIN', 'SHOW', 'USE', 'DROP', 'ALTER', 'MAP', 'SET', 'RESET', 'START', 'COMMIT', 'ROLLBACK', 'REDUCE', 'REFRESH', 'CLEAR', 'CACHE', 'UNCACHE', 'DFS', 'TRUNCATE', 'ANALYZE', 'LIST', 'REVOKE', 'GRANT', 'LOCK', 'UNLOCK', 'MSCK', 'EXPORT', 'IMPORT', 'LOAD'}(line 1, pos 0)

== SQL ==
WHERE p_month_id='201803' and p_day_id='20180325' and lan_id = '11' limit 5
^^^

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)


Error: org.apache.spark.sql.AnalysisException: cannot resolve '`p_month_id`' given input columns: []; line 3 pos 9;
'GlobalLimit 5
+- 'LocalLimit 5
   +- 'Project ['prod_inst_id, 'STATUS_DATE AS FROM#62083]
      +- 'Filter ((('p_month_id = 201803) && ('p_day_id = 20180325)) && ('lan_id = 11))
         +- OneRowRelation$ (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: cannot resolve '`p_month_id`' given input columns: []; line 3 pos 9;
'GlobalLimit 5
+- 'LocalLimit 5
   +- 'Project ['prod_inst_id, 'STATUS_DATE AS FROM#62083]
      +- 'Filter ((('p_month_id = 201803) && ('p_day_id = 20180325)) && ('lan_id = 11))
         +- OneRowRelation$

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)


Error: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'SELECT' expecting {<EOF>, ',', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'JOIN', 'CROSS', 'INNER', 'LEFT', 'RIGHT', 'FULL', 'NATURAL', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'ANTI'}(line 1, pos 42)

== SQL ==
select * from ass_evt_cdma_active_d where SELECT prod_inst_id,STATUS_DATE
------------------------------------------^^^
    FROM    --取全量表数据,接口已为全量数据
   WHERE p_month_id='201803' and p_day_id='20180325' and lan_id = '11' (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'SELECT' expecting {<EOF>, ',', 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'JOIN', 'CROSS', 'INNER', 'LEFT', 'RIGHT', 'FULL', 'NATURAL', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE', 'ANTI'}(line 1, pos 42)

== SQL ==
select * from ass_evt_cdma_active_d where SELECT prod_inst_id,STATUS_DATE
------------------------------------------^^^
    FROM    --取全量表数据,接口已为全量数据
   WHERE p_month_id='201803' and p_day_id='20180325' and lan_id = '11'

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)
    
    
    
Error: org.apache.spark.sql.AnalysisException: cannot resolve '`p_day_id`' given input columns: []; line 29 pos 9;
'Project ['ORDER_ITEM_ID, 'CUST_ORDER_ID, 'OFFER_INST_ID, 'OFFER_TYPE, 'SRC_OFFER_ID, 'STD_OFFER_ID, 'ACCEPT_MODE, 'ACTION_TYPE, 'UNDO_FLAG, 'SRC_ORDER_STATUS_CD, 'STD_ORDER_STATUS_CD, 'STD_PROD_ID, 'SRC_PROD_ID, 'prod_inst_id, 'acc_nbr, 'ADDRESS_ID, 'ADDRESS_DESC, 'CREATE_STAFF_ID, 'DEV_STAFF_ID, 'ACCEPT_DATE, 'FINISH_DATE, 'SRC_SERVICE_OFFER_ID, 'CHANNEL_ID, 'SRC_REGION_ID, 'CREATE_POST AS FROM#58196]
+- 'Filter (('p_day_id = 20180325) && ('p_lan_id = 11))
   +- OneRowRelation$ (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: cannot resolve '`p_day_id`' given input columns: []; line 29 pos 9;
'Project ['ORDER_ITEM_ID, 'CUST_ORDER_ID, 'OFFER_INST_ID, 'OFFER_TYPE, 'SRC_OFFER_ID, 'STD_OFFER_ID, 'ACCEPT_MODE, 'ACTION_TYPE, 'UNDO_FLAG, 'SRC_ORDER_STATUS_CD, 'STD_ORDER_STATUS_CD, 'STD_PROD_ID, 'SRC_PROD_ID, 'prod_inst_id, 'acc_nbr, 'ADDRESS_ID, 'ADDRESS_DESC, 'CREATE_STAFF_ID, 'DEV_STAFF_ID, 'ACCEPT_DATE, 'FINISH_DATE, 'SRC_SERVICE_OFFER_ID, 'CHANNEL_ID, 'SRC_REGION_ID, 'CREATE_POST AS FROM#58196]
+- 'Filter (('p_day_id = 20180325) && ('p_lan_id = 11))
   +- OneRowRelation$

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467) 
    
    
 Error: org.apache.spark.sql.AnalysisException: cannot resolve '`p_day_id`' given input columns: [address_id, src_service_offer_id, channel_id, acc_nbr, dev_staff_id, finish_date, cust_order_id, undo_flag, create_staff_id, src_offer_id, offer_type, std_offer_id, offer_inst_id, action_type, accept_date, prod_inst_id, order_item_id, create_post, src_order_status_cd, std_order_status_cd, accept_mode, src_prod_id, src_region_id, address_desc, std_prod_id]; line 1 pos 54;
'Aggregate [unresolvedalias(count(1), None)]
+- 'Filter (('p_day_id = 20180325) && ('p_lan_id = 11))
   +- MetastoreRelation tmp, tmp_ass_evt_order_item (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.AnalysisException: cannot resolve '`p_day_id`' given input columns: [address_id, src_service_offer_id, channel_id, acc_nbr, dev_staff_id, finish_date, cust_order_id, undo_flag, create_staff_id, src_offer_id, offer_type, std_offer_id, offer_inst_id, action_type, accept_date, prod_inst_id, order_item_id, create_post, src_order_status_cd, std_order_status_cd, accept_mode, src_prod_id, src_region_id, address_desc, std_prod_id]; line 1 pos 54;
'Aggregate [unresolvedalias(count(1), None)]
+- 'Filter (('p_day_id = 20180325) && ('p_lan_id = 11))
   +- MetastoreRelation tmp, tmp_ass_evt_order_item

        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)   
    
 Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'ass_evt_order_item_d' not found in database 'cfg'; (state=,code=0)
java.sql.SQLException: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'ass_evt_order_item_d' not found in database 'cfg';
        at org.apache.hive.jdbc.HiveStatement.execute(HiveStatement.java:296)
        at org.apache.hive.beeline.Commands.execute(Commands.java:848)
        at org.apache.hive.beeline.Commands.sql(Commands.java:713)
        at org.apache.hive.beeline.BeeLine.dispatch(BeeLine.java:973)
        at org.apache.hive.beeline.BeeLine.execute(BeeLine.java:813)
        at org.apache.hive.beeline.BeeLine.begin(BeeLine.java:771)
        at org.apache.hive.beeline.BeeLine.mainWithInputRedirection(BeeLine.java:484)
        at org.apache.hive.beeline.BeeLine.main(BeeLine.java:467)




Error: org.apache.spark.sql.AnalysisException: cannot resolve '`AS`' given input columns: [cust_order_id, status_cd, ACCEPT_DATE, std_offer_id, SRC_SERVICE_OFFER_ID, FINISH_DATE, prod_inst_id, acc_nbr, src_region_id, CREATE_STAFF_ID, undo_flag, status_date, ACTION_TYPE, std_lan_id, SRC_REGION_ID, std_region_name, src_offer_id, order_item_id, rn, std_region_id, ADDRESS_DESC, SRC_PROD_ID, DEV_STAFF_ID, std_ORDER_STATUS_CD, src_lan_id, STD_PROD_ID, src_region_name, ADDRESS_ID, CHANNEL_ID, OFFER_INST_ID, SRC_ORDER_STATUS_CD, ACCEPT_MODE, CREATE_POST]; line 39 pos 71;
'InsertIntoTable 'UnresolvedRelation `TST`.`ASS_EVT_OFR_INST_D`, Map(p_day_id -> Some(20180325), p_lan_id -> Some(11)), OverwriteOptions(true,Map(p_day_id -> 20180325, p_lan_id -> 11)), false
+- 'Project [coalesce(trim(order_item_id#55200), -1) AS ORDER_ITEM_ID#55177, coalesce(trim(cust_order_id#55201), -1) AS cust_order_id#55178, coalesce(trim(prod_inst_id#55213), -1) AS prod_inst_id#55179, coalesce(trim(OFFER_INST_ID#55202), -1) AS OFFER_INST_ID#55180, coalesce(trim(SRC_OFFER_ID#55204), -1) AS SRC_OFFER_ID#55181, coalesce(trim(STD_OFFER_ID#55205), -1) AS STD_OFFER_ID#55182, CASE WHEN (((accept_mode#55206 = Q) && trim(STD_PROD_ID#55211) LIKE 20%) && (SRC_SERVICE_OFFER_ID#55221 = 1)) THEN CRM77 WHEN (ACTION_TYPE#55207 = A) THEN CRM49 WHEN (ACTION_TYPE#55207 = D) THEN CRM50 END AS SRC_SERVICE_OFFER_ID#55183, CASE WHEN (ACTION_TYPE#55207 = A) THEN 10 WHEN (ACTION_TYPE#55207 = D) THEN 13 END AS STD_SERVICE_OFFER_ID#55184, coalesce(trim(SRC_ORDER_STATUS_CD#55209), NA) AS SRC_ORDER_STATUS_CD#55185, CASE WHEN (UNDO_FLAG#55208 = T) THEN 13 WHEN isnotnull(STD_ORDER_STATUS_CD#55210) THEN trim(STD_ORDER_STATUS_CD#55210) ELSE -1 END AS STD_ORDER_STATUS_CD#55186, coalesce(trim(CHANNEL_ID#55222), -1) AS CHANNEL_ID#55187, coalesce(trim(STD_PROD_ID#55211), -1) AS STD_PROD_ID#55188, coalesce(ACC_NBR#55214, NA) AS ACC_NBR#55189, coalesce(ADDRESS_DESC#55216, NA) AS ADDRESS_DESC#55190, coalesce(trim(DEV_STAFF_ID#55218), -1) AS DEV_STAFF_ID#55191, coalesce(trim(CREATE_STAFF_ID#55217), -1) AS CREATE_STAFF_ID#55192, coalesce(std_region_id#55227, -1) AS STD_REGION_ID#55193, coalesce(ACCEPT_DATE#55219, from_unixtime(unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss), yyyyMMddHHmmss)) AS ACCEPT_DATE#55194, coalesce(FINISH_DATE#55220, from_unixtime(unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss), yyyyMMddHHmmss)) AS FINISH_DATE#55195, SRC_PROD_ID#55212, 20180325 AS DAY_ID#55196, 'AS AS MON_ID#55197, from_unixtime(unix_timestamp(current_timestamp(), yyyy-MM-dd HH:mm:ss), yyyyMMddHHmmss) AS etl_time#55198, 11 AS lan_id#55199, ... 2 more fields]
   +- Filter (rn#55175 = 1)
      +- Join LeftOuter, ((STATUS_CD#55231 = 00A) && (coalesce(trim(src_region_id#55223), -1) = coalesce(trim(src_region_id#55225), -1)))
         :- SubqueryAlias a1
         :  +- Project [order_item_id#55200, cust_order_id#55201, OFFER_INST_ID#55202, src_offer_id#55204, std_offer_id#55205, ACCEPT_MODE#55206, ACTION_TYPE#55207, undo_flag#55208, SRC_ORDER_STATUS_CD#55209, std_ORDER_STATUS_CD#55210, STD_PROD_ID#55211, SRC_PROD_ID#55212, prod_inst_id#55213, acc_nbr#55214, ADDRESS_DESC#55216, CREATE_STAFF_ID#55217, DEV_STAFF_ID#55218, ACCEPT_DATE#55219, FINISH_DATE#55220, SRC_SERVICE_OFFER_ID#55221, CHANNEL_ID#55222, SRC_REGION_ID#55223, CREATE_POST#55224, ADDRESS_ID#55215, rn#55175]
         :     +- Project [order_item_id#55200, cust_order_id#55201, OFFER_INST_ID#55202, src_offer_id#55204, std_offer_id#55205, ACCEPT_MODE#55206, ACTION_TYPE#55207, undo_flag#55208, SRC_ORDER_STATUS_CD#55209, std_ORDER_STATUS_CD#55210, STD_PROD_ID#55211, SRC_PROD_ID#55212, prod_inst_id#55213, acc_nbr#55214, ADDRESS_DESC#55216, CREATE_STAFF_ID#55217, DEV_STAFF_ID#55218, ACCEPT_DATE#55219, FINISH_DATE#55220, SRC_SERVICE_OFFER_ID#55221, CHANNEL_ID#55222, SRC_REGION_ID#55223, CREATE_POST#55224, ADDRESS_ID#55215, ... 2 more fields]
         :        +- Window [row_number() windowspecdefinition(prod_inst_id#55213, SRC_ORDER_STATUS_CD#55209, SRC_SERVICE_OFFER_ID#55221, SRC_OFFER_ID#55204, FINISH_DATE#55220, ACTION_TYPE#55207, ORDER_ITEM_ID#55200, OFFER_INST_ID#55202, FINISH_DATE#55220 DESC NULLS LAST, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS rn#55175], [prod_inst_id#55213, SRC_ORDER_STATUS_CD#55209, SRC_SERVICE_OFFER_ID#55221, SRC_OFFER_ID#55204, FINISH_DATE#55220, ACTION_TYPE#55207, ORDER_ITEM_ID#55200, OFFER_INST_ID#55202], [FINISH_DATE#55220 DESC NULLS LAST]
         :           +- Project [order_item_id#55200, cust_order_id#55201, OFFER_INST_ID#55202, src_offer_id#55204, std_offer_id#55205, ACCEPT_MODE#55206, ACTION_TYPE#55207, undo_flag#55208, SRC_ORDER_STATUS_CD#55209, std_ORDER_STATUS_CD#55210, STD_PROD_ID#55211, SRC_PROD_ID#55212, prod_inst_id#55213, acc_nbr#55214, ADDRESS_DESC#55216, CREATE_STAFF_ID#55217, DEV_STAFF_ID#55218, ACCEPT_DATE#55219, FINISH_DATE#55220, SRC_SERVICE_OFFER_ID#55221, CHANNEL_ID#55222, SRC_REGION_ID#55223, CREATE_POST#55224, ADDRESS_ID#55215]
         :              +- Filter ((NOT (offer_type#55203 = 10) && action_type#55207 IN (A,D)) && (NOT SRC_OFFER_ID#55204 IN (1131049,1130305,1131050,1133385,1133386) && NOT predicate-subquery#55176 [(OFFER_INST_ID#55202 = comp_inst_id#55234)]))
         :                 :  +- Project [1 AS 1#55235, comp_inst_id#55234]
         :                 :     +- SubqueryAlias a3
         :                 :        +- MetastoreRelation tmp, tmp_access_prod_inst_xm
         :                 +- SubqueryAlias t
         :                    +- MetastoreRelation tmp, tmp_ass_evt_order_item
         +- SubqueryAlias a2
            +- MetastoreRelation cfg, map_std_region (state=,code=0)



Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'ass_prd_prod_inst_d' not found in database 'default'; (state=,code=0)

Error: org.apache.spark.sql.catalyst.analysis.NoSuchTableException: Table or view 'mid_prod_inst_his_l' not found in database 'tst'; (state=,code=0)

Error: org.apache.spark.sql.AnalysisException: cannot resolve '`IS_IN_JZ_USER`' given input columns: [channel_id, ty_type, mon_id, src_offer_id, is_trans, prod_inst_id, std_prod_id, balance, is_lj, p_day_id, day_id, dev_staff_id, dev_date, lan_id, acc_nbr, etl_time, create_staff_id, trns_date, p_lan_id, offer_inst_id]; line 9 pos 1;
'Project [DAY_ID#33372, MON_ID#33373, LAN_ID#33374, ETL_TIME#33375, PROD_INST_ID#33376, ACC_NBR#33377, 'IS_IN_JZ_USER, 'CHANNEL_DZS_ID, STD_PROD_ID#33379, 'STD_REGION_ID, CHANNEL_ID#33387]
+- Filter ((P_DAY_ID#33370 = 20180415) && (DAY_ID#33372 = 20180415))
   +- SubqueryAlias A2
      +- MetastoreRelation tst, lab_evt_prd_ty_user_share_d (state=,code=0)

Error: org.apache.spark.sql.AnalysisException: cannot resolve '`A.BEG_TIME`' given input columns: [offer_type, promise_cycle, etl_time, exp_date, eff_date, pricing_plan_id, end_month, src_offer_id, offer_obj_inst_rel_id, mon_id, std_prod_id, is_trans, end_time, create_staff_id, day_id, bt_type, status_date, obj_type, return_fee_month, dev_staff_id, acc_nbr, prod_inst_id, status_cd, offer_grade, balance, offer_inst_id, p_day_id, dev_date, ext_offer_inst_id, mon_id, brand_id, trns_date, ty_type, beg_time, offer_inst_id, p_lan_id, std_offer_id, bt_amount, day_id, owner_cust_id, src_offer_name, special_type, ext_offer_id, src_offer_id, eff_date, lan_id, is_lj, deposit_amount, offer_nbr, exp_date, main_flag, offer_sys_type, p_lan_id, offer_desc, lan_id, p_day_id, status_cd, obj_id, common_type, etl_time, lan_id, prod_comp_role_cd, src_offer_id, channel_id, etl_time]; line 11 pos 13;
'InsertIntoTable 'UnresolvedRelation `TMP`.`TMP_LAB_EVT_PRD_TY_USER_SHARE_D02`, OverwriteOptions(false,Map()), false
+- 'Project ['PROD_INST_ID, 'BALANCE, 'DAY_ID, 'LAN_ID]
   +- 'Filter ('RN = 1)
      +- 'SubqueryAlias X
         +- 'Project [OBJ_ID#32431 AS PROD_INST_ID#32418, 0 AS BALANCE#32419, 'SUBSTR('A.BEG_TIME, 1, 8) AS DAY_ID#32420, row_number() windowspecdefinition(OBJ_ID#32431, EFF_DATE#32438 ASC NULLS FIRST, ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS RN#32421, LAN_ID#32427]
            +- Filter ((((P_DAY_ID#32423 = 20180415) && NOT predicate-subquery#32422 [(OBJ_ID#32431 = PROD_INST_ID#32489)]) && (MAIN_FLAG#32459 = 1)) && (NOT (SRC_OFFER_ID#32434 = 1172679) && (OBJ_TYPE#32432 = 100000)))
               :  +- Project [PROD_INST_ID#32489]
               :     +- MetastoreRelation tmp, tmp_lab_evt_prd_ty_user_share_d02
               +- Join Inner, ((((OBJ_ID#32431 = PROD_INST_ID#32474) && (LAN_ID#32427 = LAN_ID#32472)) && (P_DAY_ID#32468 = 20180415)) && (((cast(IS_TRANS#32480 as double) = cast(0 as double)) && STD_PROD_ID#32477 LIKE 1303%) && (cast(TY_TYPE#32479 as double) = cast(1 as double))))
                  :- Join Inner, (SRC_OFFER_ID#32434 = SRC_OFFER_ID#32451)
                  :  :- SubqueryAlias A
                  :  :  +- MetastoreRelation tst, ass_ofr_offer_inst_d
                  :  +- SubqueryAlias B
                  :     +- MetastoreRelation cfg, cfg_src_offer_l
                  +- SubqueryAlias C
                     +- MetastoreRelation tst, lab_evt_prd_ty_user_share_d (state=,code=0)

Error: org.apache.spark.sql.catalyst.parser.ParseException: 
mismatched input 'FROM' expecting {<EOF>, 'WHERE', 'GROUP', 'ORDER', 'HAVING', 'LIMIT', 'LATERAL', 'WINDOW', 'UNION', 'EXCEPT', 'MINUS', 'INTERSECT', 'SORT', 'CLUSTER', 'DISTRIBUTE'}(line 7, pos 0)

== SQL ==
CREATE TABLE TMP.TMP_LAB_EVT_PRD_TY_USER_SHARE_D02 AS
SELECT
      PROD_INST_ID
     ,BALANCE
     ,DAY_ID
     ,LAN_ID
FROM(
^^^
    SELECT
     B.PROD_INST_ID  AS PROD_INST_ID
    ,A.BALANCE       AS BALANCE
    ,A.DAY_ID        AS DAY_ID
    ,ROW_NUMBER() OVER (PARTITION BY B.PROD_INST_ID ORDER BY A.DAY_ID ASC) RN
    ,A.LAN_ID
    FROM TST.LAB_BIL_PROD_BALANCE_D A
    INNER JOIN TST.LAB_EVT_PRD_TY_USER_SHARE_D B
    ON A.PROD_INST_ID = B.PROD_INST_ID
    AND B.LAN_ID=A.LAN_ID
    AND B.P_DAY_ID='20180415'
    AND B.TY_TYPE IN (2,3,5)--1：体验宽带，itv；2：体验任性卡,3：宽带老用户续费加C送话费
    AND B.IS_TRANS = 0
    WHERE A.P_DAY_ID='20180415'
    AND A.PROD_INST_ID IN
    (SELECT PROD_INST_ID FROM TST.LAB_EVT_PRD_TY_USER_SHARE_D WHERE P_DAY_ID='20180415' AND TY_TYPE IN (2,3,5) AND IS_TRANS = 0)
    AND (A.BALANCE >=900 AND B.SRC_OFFER_ID IN ('1176356','1177498') OR A.BALANCE >=3900
    AND B.SRC_OFFER_ID IN ('1176441','1181733') OR A.BALANCE >0
    AND B.SRC_OFFER_ID IN ('1176326','1184758') ) --201604-视频体验卡201604 0元套餐 (state=,code=0)
    
    
    





        